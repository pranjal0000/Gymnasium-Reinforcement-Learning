{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Reward: 16.00, Epsilon: 1.0000\n",
      "Episode 10, Reward: 67.00, Epsilon: 0.9511\n",
      "Episode 20, Reward: 22.00, Epsilon: 0.9046\n",
      "Episode 30, Reward: 18.00, Epsilon: 0.8604\n",
      "Episode 40, Reward: 21.00, Epsilon: 0.8183\n",
      "Episode 50, Reward: 15.00, Epsilon: 0.7783\n",
      "Episode 60, Reward: 23.00, Epsilon: 0.7403\n",
      "Episode 70, Reward: 9.00, Epsilon: 0.7041\n",
      "Episode 80, Reward: 12.00, Epsilon: 0.6696\n",
      "Episode 90, Reward: 12.00, Epsilon: 0.6369\n",
      "Episode 100, Reward: 16.00, Epsilon: 0.6058\n",
      "Episode 110, Reward: 21.00, Epsilon: 0.5762\n",
      "Episode 120, Reward: 12.00, Epsilon: 0.5480\n",
      "Episode 130, Reward: 15.00, Epsilon: 0.5212\n",
      "Episode 140, Reward: 13.00, Epsilon: 0.4957\n",
      "Episode 150, Reward: 12.00, Epsilon: 0.4715\n",
      "Episode 160, Reward: 10.00, Epsilon: 0.4484\n",
      "Episode 170, Reward: 18.00, Epsilon: 0.4265\n",
      "Episode 180, Reward: 16.00, Epsilon: 0.4057\n",
      "Episode 190, Reward: 9.00, Epsilon: 0.3858\n",
      "Episode 200, Reward: 15.00, Epsilon: 0.3670\n",
      "Episode 210, Reward: 13.00, Epsilon: 0.3490\n",
      "Episode 220, Reward: 11.00, Epsilon: 0.3320\n",
      "Episode 230, Reward: 10.00, Epsilon: 0.3157\n",
      "Episode 240, Reward: 13.00, Epsilon: 0.3003\n",
      "Episode 250, Reward: 12.00, Epsilon: 0.2856\n",
      "Episode 260, Reward: 15.00, Epsilon: 0.2716\n",
      "Episode 270, Reward: 9.00, Epsilon: 0.2584\n",
      "Episode 280, Reward: 9.00, Epsilon: 0.2457\n",
      "Episode 290, Reward: 15.00, Epsilon: 0.2337\n",
      "Episode 300, Reward: 11.00, Epsilon: 0.2223\n",
      "Episode 310, Reward: 12.00, Epsilon: 0.2114\n",
      "Episode 320, Reward: 18.00, Epsilon: 0.2011\n",
      "Episode 330, Reward: 13.00, Epsilon: 0.1913\n",
      "Episode 340, Reward: 15.00, Epsilon: 0.1819\n",
      "Episode 350, Reward: 41.00, Epsilon: 0.1730\n",
      "Episode 360, Reward: 9.00, Epsilon: 0.1646\n",
      "Episode 370, Reward: 40.00, Epsilon: 0.1565\n",
      "Episode 380, Reward: 22.00, Epsilon: 0.1489\n",
      "Episode 390, Reward: 38.00, Epsilon: 0.1416\n",
      "Episode 400, Reward: 19.00, Epsilon: 0.1347\n",
      "Episode 410, Reward: 11.00, Epsilon: 0.1281\n",
      "Episode 420, Reward: 36.00, Epsilon: 0.1218\n",
      "Episode 430, Reward: 16.00, Epsilon: 0.1159\n",
      "Episode 440, Reward: 9.00, Epsilon: 0.1102\n",
      "Episode 450, Reward: 17.00, Epsilon: 0.1048\n",
      "Episode 460, Reward: 38.00, Epsilon: 0.0997\n",
      "Episode 470, Reward: 12.00, Epsilon: 0.0948\n",
      "Episode 480, Reward: 11.00, Epsilon: 0.0902\n",
      "Episode 490, Reward: 14.00, Epsilon: 0.0858\n",
      "Episode 500, Reward: 14.00, Epsilon: 0.0816\n",
      "Episode 510, Reward: 10.00, Epsilon: 0.0776\n",
      "Episode 520, Reward: 18.00, Epsilon: 0.0738\n",
      "Episode 530, Reward: 13.00, Epsilon: 0.0702\n",
      "Episode 540, Reward: 10.00, Epsilon: 0.0668\n",
      "Episode 550, Reward: 10.00, Epsilon: 0.0635\n",
      "Episode 560, Reward: 8.00, Epsilon: 0.0604\n",
      "Episode 570, Reward: 11.00, Epsilon: 0.0574\n",
      "Episode 580, Reward: 11.00, Epsilon: 0.0546\n",
      "Episode 590, Reward: 11.00, Epsilon: 0.0520\n",
      "Episode 600, Reward: 49.00, Epsilon: 0.0494\n",
      "Episode 610, Reward: 11.00, Epsilon: 0.0470\n",
      "Episode 620, Reward: 69.00, Epsilon: 0.0447\n",
      "Episode 630, Reward: 11.00, Epsilon: 0.0425\n",
      "Episode 640, Reward: 22.00, Epsilon: 0.0404\n",
      "Episode 650, Reward: 10.00, Epsilon: 0.0385\n",
      "Episode 660, Reward: 12.00, Epsilon: 0.0366\n",
      "Episode 670, Reward: 13.00, Epsilon: 0.0348\n",
      "Episode 680, Reward: 31.00, Epsilon: 0.0331\n",
      "Episode 690, Reward: 65.00, Epsilon: 0.0315\n",
      "Episode 700, Reward: 149.00, Epsilon: 0.0299\n",
      "Episode 710, Reward: 29.00, Epsilon: 0.0285\n",
      "Episode 720, Reward: 331.00, Epsilon: 0.0271\n",
      "Episode 730, Reward: 12.00, Epsilon: 0.0258\n",
      "Episode 740, Reward: 126.00, Epsilon: 0.0245\n",
      "Episode 750, Reward: 175.00, Epsilon: 0.0233\n",
      "Episode 760, Reward: 164.00, Epsilon: 0.0222\n",
      "Episode 770, Reward: 69.00, Epsilon: 0.0211\n",
      "Episode 780, Reward: 174.00, Epsilon: 0.0200\n",
      "Episode 790, Reward: 47.00, Epsilon: 0.0191\n",
      "Episode 800, Reward: 34.00, Epsilon: 0.0181\n",
      "Episode 810, Reward: 195.00, Epsilon: 0.0172\n",
      "Episode 820, Reward: 174.00, Epsilon: 0.0164\n",
      "Episode 830, Reward: 369.00, Epsilon: 0.0156\n",
      "Episode 840, Reward: 224.00, Epsilon: 0.0148\n",
      "Episode 850, Reward: 232.00, Epsilon: 0.0141\n",
      "Episode 860, Reward: 354.00, Epsilon: 0.0134\n",
      "Episode 870, Reward: 173.00, Epsilon: 0.0128\n",
      "Episode 880, Reward: 188.00, Epsilon: 0.0121\n",
      "Episode 890, Reward: 181.00, Epsilon: 0.0115\n",
      "Episode 900, Reward: 158.00, Epsilon: 0.0110\n",
      "Episode 910, Reward: 144.00, Epsilon: 0.0104\n",
      "Episode 920, Reward: 133.00, Epsilon: 0.0099\n",
      "Episode 930, Reward: 165.00, Epsilon: 0.0095\n",
      "Episode 940, Reward: 172.00, Epsilon: 0.0090\n",
      "Episode 950, Reward: 156.00, Epsilon: 0.0085\n",
      "Episode 960, Reward: 180.00, Epsilon: 0.0081\n",
      "Episode 970, Reward: 154.00, Epsilon: 0.0077\n",
      "Episode 980, Reward: 175.00, Epsilon: 0.0074\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self,input_size, output_size, hidden_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "        previous_size = input_size\n",
    "        for hidden_size in hidden_layers:\n",
    "            layers.append(nn.Linear(previous_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            previous_size = hidden_size\n",
    "\n",
    "        layers.append(nn.Linear(previous_size, output_size))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "class ReplayMemory():\n",
    "    def __init__(self, max_size):\n",
    "        self.memory = deque(maxlen=max_size)\n",
    "\n",
    "    def addData(self, dataPoint):\n",
    "        self.memory.append(dataPoint)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(population=self.memory, k=batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "def optimize(optimizer, loss_function, discount_factor, train_data, policyDQN, targetDQN, device):\n",
    "    states, actions, next_states, rewards, dones = zip(*train_data)\n",
    "\n",
    "    # Convert to numpy arrays first then tenors\n",
    "    states = torch.tensor(np.array(states), dtype=torch.float32, device=device)\n",
    "    next_states = torch.tensor(np.array(next_states), dtype=torch.float32, device=device)\n",
    "    actions = torch.tensor(actions, dtype=torch.long, device=device)\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32, device=device)\n",
    "    dones = torch.tensor(dones, dtype=torch.float32, device=device)\n",
    "\n",
    "    current_q = policyDQN(states)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        next_q = targetDQN(next_states).max(1)[0]\n",
    "        target_values = rewards + discount_factor * next_q * (1 - dones)\n",
    "\n",
    "    current_q_selected = current_q[torch.arange(len(states)), actions]\n",
    "\n",
    "    loss = loss_function(current_q_selected, target_values)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(policyDQN.parameters(), max_norm=5.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "def stateToDQNInput(state, device):\n",
    "    state = np.array(state)\n",
    "    state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "    return state_tensor.to(device)\n",
    "\n",
    "def deepQLearning(epsInit, epsDecay, epsEnd, numEpisodes, stepsToSync, discountFactor, \n",
    "                  learningRate, minibatchSize, episodesBeforeReplay):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    env = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "    obs_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    policyDQN = DQN(input_size=obs_size, output_size=action_size, hidden_layers=[64 * obs_size, 64 * obs_size]).to(device)\n",
    "    targetDQN = DQN(input_size=obs_size, output_size=action_size, hidden_layers=[64 * obs_size, 64 * obs_size]).to(device)\n",
    "    targetDQN.load_state_dict(policyDQN.state_dict())\n",
    "\n",
    "    replayMemory = ReplayMemory(max_size=50_000)\n",
    "\n",
    "    epsilon = epsInit\n",
    "\n",
    "    optimizer = optim.Adam(policyDQN.parameters(), lr=learningRate)\n",
    "    lossFunction = nn.MSELoss()\n",
    "\n",
    "    rewardsPerEpisode = []\n",
    "    epsilonHistory = []\n",
    "    step_count = 0\n",
    "    for episode in range(numEpisodes):\n",
    "        state, _ = env.reset()\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        sumOfRewards = 0\n",
    "        while (not terminated) and (not truncated):\n",
    "            if random.random() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    q_values = policyDQN(stateToDQNInput(state, device))\n",
    "                    action = q_values.argmax().item()\n",
    "\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            replayMemory.addData((state, action, next_state, reward, terminated))\n",
    "\n",
    "            sumOfRewards += reward\n",
    "            state = next_state\n",
    "            step_count += 1\n",
    "\n",
    "            if (len(replayMemory) >= minibatchSize) and (episode >= episodesBeforeReplay):\n",
    "                minibatch = replayMemory.sample(batch_size=minibatchSize)\n",
    "                optimize(optimizer=optimizer, loss_function=lossFunction, discount_factor=discountFactor,\n",
    "                         train_data=minibatch, policyDQN=policyDQN, targetDQN=targetDQN, device=device)\n",
    "                \n",
    "                if step_count > stepsToSync:\n",
    "                    targetDQN.load_state_dict(policyDQN.state_dict())\n",
    "                    step_count = 0\n",
    "\n",
    "        rewardsPerEpisode.append(sumOfRewards)\n",
    "        epsilonHistory.append(epsilon)\n",
    "\n",
    "        if episode % 10 == 0:\n",
    "            print(f\"Episode {episode}, Reward: {sumOfRewards:.2f}, Epsilon: {epsilon:.4f}\")\n",
    "\n",
    "        epsilon = max(epsEnd, epsilon * epsDecay)\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    plt.figure(1)\n",
    "\n",
    "    meanRewards = np.zeros(numEpisodes)\n",
    "    for i in range(numEpisodes):\n",
    "        meanRewards[i] = np.mean(rewardsPerEpisode[max(0, i - 100):(i + 1)])\n",
    "    plt.subplot(121)\n",
    "    plt.plot(meanRewards)\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.plot(epsilonHistory)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return policyDQN\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    trainedDQN = deepQLearning(\n",
    "        epsInit=1.0,\n",
    "        epsDecay=0.995,\n",
    "        epsEnd=0.001,\n",
    "        numEpisodes=5_000,\n",
    "        stepsToSync=5_000,\n",
    "        discountFactor=0.99,\n",
    "        learningRate=0.0005,\n",
    "        minibatchSize=128,\n",
    "        episodesBeforeReplay=32\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
